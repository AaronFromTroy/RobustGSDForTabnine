---
phase: 07-enhanced-research-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - gsd/package.json
  - gsd/scripts/scraper.js
autonomous: true

must_haves:
  truths:
    - "System can scrape static HTML documentation sites using Cheerio"
    - "System falls back to Playwright for JavaScript-rendered content"
    - "System handles rate limiting with exponential backoff and jitter"
    - "System retries failed requests up to 3 times before giving up"
  artifacts:
    - path: "gsd/package.json"
      provides: "Web scraping dependencies"
      contains: "cheerio.*playwright.*axios.*p-limit"
    - path: "gsd/scripts/scraper.js"
      provides: "Progressive enhancement web scraping"
      exports: ["scrapeContent", "scrapeWithFallback", "fetchWithRetry"]
      min_lines: 200
  key_links:
    - from: "gsd/scripts/scraper.js"
      to: "cheerio"
      via: "load() function for static HTML parsing"
      pattern: "cheerio\\.load"
    - from: "gsd/scripts/scraper.js"
      to: "playwright"
      via: "chromium.launch() for dynamic content fallback"
      pattern: "playwright\\.chromium\\.launch"
    - from: "gsd/scripts/scraper.js"
      to: "axios"
      via: "get() for HTTP requests with retry logic"
      pattern: "axios\\.get"
---

<objective>
Establish real web scraping capability to replace Phase 4's mock data implementation.

Purpose: Enable automated research to gather actual documentation from the web instead of using generateMockSearchResults(). This is the foundation for all enhanced research features.

Output: Working scraper.js module with progressive enhancement (Cheerio → Playwright fallback), exponential backoff retry logic, and proper browser cleanup.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-enhanced-research-infrastructure/07-RESEARCH.md

Existing Research Infrastructure:
@.planning/phases/04-advanced-features/04-03-SUMMARY.md
- researcher.js currently uses generateMockSearchResults() with TODO for WebSearch
- extractFindings() filters HTTPS/forums but needs real scraping backend
- Phase 4 infrastructure ready for scraper.js integration

Pattern Reference from Research:
- Progressive enhancement: Try Cheerio first (10x faster), Playwright fallback
- Retry logic: Exponential backoff (1s, 2s, 4s) with random jitter (0-1000ms)
- Browser cleanup: Always use try-finally to close Playwright browsers
- Content validation: Check content.length > 100 to detect static vs dynamic
</context>

<tasks>

<task type="auto">
  <name>Install web scraping dependencies</name>
  <files>gsd/package.json</files>
  <action>
Install web scraping libraries:

```bash
cd gsd
npm install cheerio@1.0.0 playwright@1.48.2 axios@1.6.8 p-limit@6.1.0
```

Dependencies:
- cheerio@1.0.0: Static HTML parsing (jQuery-like syntax, 10x faster than Puppeteer)
- playwright@1.48.2: Dynamic content scraping (cross-browser, Microsoft-backed)
- axios@1.6.8: HTTP client (50M+ weekly downloads, better error handling than node-fetch)
- p-limit@6.1.0: Concurrency control (prevents resource exhaustion)

After install, verify package.json has all four dependencies in dependencies object.

IMPORTANT: Do NOT install Puppeteer (Playwright is superior - cross-browser support). Do NOT use request library (deprecated since 2020).
  </action>
  <verify>
```bash
grep -E "(cheerio|playwright|axios|p-limit)" gsd/package.json
```

All four libraries should appear in dependencies section.
  </verify>
  <done>package.json contains cheerio, playwright, axios, and p-limit with correct versions. node_modules directory contains all installed packages.</done>
</task>

<task type="auto">
  <name>Create scraper.js with progressive enhancement</name>
  <files>gsd/scripts/scraper.js</files>
  <action>
Create scraper.js implementing progressive enhancement pattern from 07-RESEARCH.md:

**Export 3 functions:**

1. **scrapeContent(url, options = {})** - Main scraping function
   - Calls scrapeWithFallback() internally
   - Returns `{ method: 'static'|'dynamic', content, title, url }`
   - Handles errors with clear messages

2. **scrapeWithFallback(url)** - Progressive enhancement implementation
   - Try Cheerio first (static HTML parsing):
     - axios.get(url) with User-Agent header
     - cheerio.load(data)
     - Extract content from 'main, article, .content' selectors
     - Check content.length > 100 (validates static content exists)
     - Return `{ method: 'static', content, title }` if successful
   - Fallback to Playwright (dynamic content):
     - chromium.launch({ headless: true })
     - Use try-finally block (CRITICAL for browser cleanup - see Pitfall 1 in research)
     - page.goto(url, { waitUntil: 'networkidle' })
     - Extract content with page.textContent()
     - Always close browser in finally block
     - Return `{ method: 'dynamic', content, title }`

3. **fetchWithRetry(url, maxRetries = 3)** - Exponential backoff retry logic
   - Loop maxRetries times
   - Try axios.get(url, { timeout: 10000 })
   - On 429 rate limit error:
     - Calculate baseDelay: Math.pow(2, attempt) * 1000 (1s, 2s, 4s)
     - Add jitter: Math.random() * 1000 (prevents retry storms - see Pitfall 2)
     - Sleep totalDelay = baseDelay + jitter
     - Continue to next attempt
   - On other errors or last attempt: throw error
   - On success: return response

**Implementation requirements:**
- ESM module: Use `import` syntax
- Error handling: Wrap scraping logic in try-catch
- Logging: console.warn() for fallbacks and retries
- User-Agent: Set to 'Mozilla/5.0 (Research Bot)' to avoid bot detection
- Browser cleanup: ALWAYS use try-finally (unclosed browsers cause memory leaks)

**Anti-patterns to AVOID:**
- Puppeteer-first scraping (use Cheerio first per research)
- Missing try-finally for browser (Pitfall 1 - memory leaks)
- No jitter in retry delays (Pitfall 2 - retry storms)
- Synchronous HTTP (never use request() library)

Reference implementation patterns in 07-RESEARCH.md sections:
- "Pattern 1: Progressive Enhancement Scraping"
- "Pattern 3: Exponential Backoff with Jitter"
- "Pitfall 1: Not Closing Browser Contexts"
  </action>
  <verify>
```bash
# Check exports
grep -E "export (async )?function (scrapeContent|scrapeWithFallback|fetchWithRetry)" gsd/scripts/scraper.js

# Check critical imports
grep -E "import.*cheerio" gsd/scripts/scraper.js
grep -E "import.*playwright" gsd/scripts/scraper.js
grep -E "import.*axios" gsd/scripts/scraper.js

# Check browser cleanup (try-finally)
grep -A5 "chromium\.launch" gsd/scripts/scraper.js | grep "finally"

# Check jitter implementation
grep "Math\.random" gsd/scripts/scraper.js
```

All checks should pass - exports exist, imports present, try-finally for browser, jitter in retry logic.
  </verify>
  <done>
scraper.js exists with 3 exported functions. Progressive enhancement implemented (Cheerio → Playwright fallback). Exponential backoff with jitter for retries. Browser cleanup in try-finally block. File is 200+ lines with complete error handling.
  </done>
</task>

</tasks>

<verification>
Manual verification after task completion:

1. Dependencies installed:
   ```bash
   ls gsd/node_modules | grep -E "(cheerio|playwright|axios|p-limit)"
   ```
   All four directories should exist.

2. scraper.js structure:
   ```bash
   node -e "import('./gsd/scripts/scraper.js').then(m => console.log(Object.keys(m)))"
   ```
   Should output: ['scrapeContent', 'scrapeWithFallback', 'fetchWithRetry']

3. Quick scrape test (optional - requires network):
   ```bash
   node -e "import('./gsd/scripts/scraper.js').then(async m => { const result = await m.scrapeContent('https://nodejs.org/en/docs/'); console.log(result.method, result.content.substring(0, 100)); })"
   ```
   Should return static method and content from Node.js docs.
</verification>

<success_criteria>
1. package.json contains cheerio, playwright, axios, p-limit dependencies
2. scraper.js exports scrapeContent, scrapeWithFallback, fetchWithRetry functions
3. Progressive enhancement implemented (Cheerio first, Playwright fallback)
4. Exponential backoff with jitter prevents retry storms
5. Browser cleanup always executes via try-finally block
6. User-Agent header set to avoid bot detection
7. Content validation checks length > 100 to detect JavaScript-rendered pages
8. Error messages are clear and actionable
</success_criteria>

<output>
After completion, create `.planning/phases/07-enhanced-research-infrastructure/07-01-SUMMARY.md`

Summary should document:
- Dependencies installed and versions
- scraper.js functions and signatures
- Progressive enhancement pattern verified
- Retry logic tested with rate limiting scenarios
- Browser cleanup verified (no memory leaks)
- Any deviations from research patterns
</output>
