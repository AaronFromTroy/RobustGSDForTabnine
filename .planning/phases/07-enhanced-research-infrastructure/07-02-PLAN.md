---
phase: 07-enhanced-research-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - gsd/scripts/source-validator.js
  - gsd/scripts/deduplicator.js
autonomous: true

must_haves:
  truths:
    - "System classifies sources into HIGH/MEDIUM/LOW/UNVERIFIED authority tiers"
    - "System detects duplicate content even when URLs differ"
    - "System merges alternate sources for same content"
    - "Authority classification uses regex patterns not string matching"
  artifacts:
    - path: "gsd/scripts/source-validator.js"
      provides: "Multi-tier source authority classification"
      exports: ["classifySourceAuthority", "assignConfidenceLevel"]
      min_lines: 100
    - path: "gsd/scripts/deduplicator.js"
      provides: "Content-based deduplication with hashing"
      exports: ["deduplicateFindings", "hashContent"]
      min_lines: 80
  key_links:
    - from: "gsd/scripts/source-validator.js"
      to: "AUTHORITY_RULES patterns"
      via: "regex matching for tier classification"
      pattern: "AUTHORITY_RULES.*HIGH.*MEDIUM.*LOW"
    - from: "gsd/scripts/deduplicator.js"
      to: "crypto.createHash"
      via: "SHA256 hashing for content comparison"
      pattern: "crypto\\.createHash\\('sha256'\\)"
---

<objective>
Enhance research quality with improved source validation and content deduplication.

Purpose: Replace Phase 4's basic domain matching (docs.*, .dev) with multi-tier authority classification. Add content-based deduplication to catch duplicate content from different URLs (versioned docs, localized pages, canonical URLs).

Output: source-validator.js with regex-based authority rules and deduplicator.js with SHA256 content hashing.
</objective>

<execution_context>
@~/.claude/get-stuff-done/workflows/execute-plan.md
@~/.claude/get-stuff-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-enhanced-research-infrastructure/07-RESEARCH.md

Phase 4 Existing Implementation:
@.planning/phases/04-advanced-features/04-02-SUMMARY.md
- research-synthesizer.js has assignConfidenceLevel() with simple domain matching
- Currently checks: docs.*, .dev, github.com/*/docs/ = HIGH confidence
- URL-only deduplication in researcher.js (misses duplicate content with different URLs)

Enhancement Targets from Research:
- Pattern 4: Source Authority Classification (multi-tier with regex)
- Pattern 5: Content-Based Deduplication (SHA256 hashing)
- Pitfall 5: URL-Only Deduplication catches same content on different URLs
</context>

<tasks>

<task type="auto">
  <name>Create source-validator.js with multi-tier authority</name>
  <files>gsd/scripts/source-validator.js</files>
  <action>
Create source-validator.js implementing enhanced authority classification from 07-RESEARCH.md:

**Export 2 functions:**

1. **classifySourceAuthority(url)** - Multi-tier classification
   - Define AUTHORITY_RULES constant with regex patterns:
     - HIGH tier: /^https:\/\/docs\./, /^https:\/\/[^/]+\.dev\//, /\/official\//, /github\.com\/[^/]+\/[^/]+\/docs\//, /^https:\/\/[^/]+\.org\/docs\//
     - MEDIUM tier: /developer\.mozilla\.org/, /stackoverflow\.com/, /\.edu\//, /\.gov\//
     - LOW tier: /medium\.com/, /dev\.to/, /hashnode\.dev/, /blog\./
   - Iterate through tiers in order (HIGH → MEDIUM → LOW)
   - Test each pattern against url.toLowerCase()
   - Return tier name on first match
   - Return 'UNVERIFIED' if no matches

2. **assignConfidenceLevel(finding)** - Enhanced confidence with metadata
   - Call classifySourceAuthority(finding.source)
   - Check finding.verifiedWithOfficial flag (if present)
   - Return 'HIGH' if authority === 'HIGH' OR verifiedWithOfficial === true
   - Return 'MEDIUM' if authority === 'MEDIUM'
   - Return 'LOW' if authority === 'LOW'
   - Return 'UNVERIFIED' otherwise

**Implementation requirements:**
- ESM module: Use export syntax
- AUTHORITY_RULES as const at module level (not inside function)
- Regex patterns: Use regex objects (/pattern/) NOT string matching
- Case-insensitive: url.toLowerCase() before testing
- Object.entries() iteration for clean tier lookup

**Why regex over string matching:**
- More flexible: Can match patterns like github.com/*/docs/ (wildcard)
- More precise: /^https:\/\/docs\./ only matches docs.* at start, not anywhere
- Extensible: Easy to add complex patterns like /api\.([^/]+)\.(com|org|dev)\/docs\//

Reference: 07-RESEARCH.md "Pattern 4: Source Authority Classification"
  </action>
  <verify>
```bash
# Check exports
grep -E "export (const|function) (classifySourceAuthority|assignConfidenceLevel)" gsd/scripts/source-validator.js

# Check AUTHORITY_RULES structure
grep -A10 "AUTHORITY_RULES" gsd/scripts/source-validator.js | grep -E "(HIGH|MEDIUM|LOW)"

# Check regex patterns (not strings)
grep "AUTHORITY_RULES" gsd/scripts/source-validator.js | grep -E "\/.*\/"

# Quick test
node -e "import('./gsd/scripts/source-validator.js').then(m => console.log(m.classifySourceAuthority('https://docs.example.com/guide')))"
```

Should output: HIGH
  </verify>
  <done>
source-validator.js exists with 2 exported functions. AUTHORITY_RULES defined with regex patterns for 3 tiers. classifySourceAuthority uses regex matching (not string contains). assignConfidenceLevel enhances confidence with metadata. File is 100+ lines.
  </done>
</task>

<task type="auto">
  <name>Create deduplicator.js with content hashing</name>
  <files>gsd/scripts/deduplicator.js</files>
  <action>
Create deduplicator.js implementing content-based deduplication from 07-RESEARCH.md:

**Export 2 functions:**

1. **hashContent(content)** - Normalize and hash content
   - Normalize content: content.toLowerCase().replace(/\s+/g, ' ').trim()
     - Lowercase: Ignore case differences
     - Collapse whitespace: "hello  world\n" becomes "hello world"
     - Trim: Remove leading/trailing spaces
   - Create SHA256 hash: crypto.createHash('sha256').update(normalized).digest('hex')
   - Return hex string (64 characters)

2. **deduplicateFindings(findings)** - Remove duplicate content
   - Create Map: hash → first finding
   - Iterate through findings array
   - For each finding:
     - Hash content using hashContent()
     - If hash NOT in Map:
       - Add to Map and deduplicated array
     - If hash ALREADY in Map (duplicate):
       - Retrieve existing finding
       - Add current finding's source to existing.alternateSources array
       - Format: `{ url: finding.source, title: finding.title }`
   - Log deduplication stats: `${findings.length} → ${deduplicated.length} findings`
   - Return deduplicated array

**Implementation requirements:**
- ESM module: Use import/export syntax
- Import crypto from 'node:crypto' (built-in, no dependency)
- Use Map (not object) for seen tracking (better performance)
- Preserve all metadata from first occurrence of duplicate content
- alternateSources: Initialize as empty array on first duplicate, append subsequent
- Logging: console.log() to show deduplication effectiveness

**Why content hashing vs URL comparison:**
- Official docs often have multiple URLs for same content:
  - Versioned: /v1.0/guide, /v2.0/guide (same content)
  - Localized: /en/guide, /guide (English default)
  - Canonical: /docs/guide, /documentation/guide
- Catches near-duplicates with minor formatting differences
- SHA256 is fast and collision-resistant (crypto-safe)

Reference: 07-RESEARCH.md "Pattern 5: Content-Based Deduplication" and "Pitfall 5: URL-Only Deduplication"
  </action>
  <verify>
```bash
# Check exports
grep -E "export (const|function) (deduplicateFindings|hashContent)" gsd/scripts/deduplicator.js

# Check crypto import
grep "import.*crypto.*node:crypto" gsd/scripts/deduplicator.js

# Check SHA256 usage
grep "createHash.*sha256" gsd/scripts/deduplicator.js

# Check normalization
grep "toLowerCase.*replace.*trim" gsd/scripts/deduplicator.js

# Quick test
node -e "import('./gsd/scripts/deduplicator.js').then(m => console.log(m.hashContent('Hello  World')))"
```

Should output a 64-character hex string.
  </verify>
  <done>
deduplicator.js exists with 2 exported functions. hashContent normalizes and hashes with SHA256. deduplicateFindings uses Map for tracking, merges alternate sources, logs stats. crypto imported from node:crypto. File is 80+ lines with comprehensive deduplication logic.
  </done>
</task>

</tasks>

<verification>
Manual verification after task completion:

1. source-validator.js classification:
   ```bash
   node -e "import('./gsd/scripts/source-validator.js').then(async m => {
     console.log('docs:', m.classifySourceAuthority('https://docs.react.dev/'));
     console.log('mdn:', m.classifySourceAuthority('https://developer.mozilla.org/'));
     console.log('medium:', m.classifySourceAuthority('https://medium.com/article'));
     console.log('unknown:', m.classifySourceAuthority('https://random.com/page'));
   })"
   ```
   Expected: HIGH, MEDIUM, LOW, UNVERIFIED

2. deduplicator.js functionality:
   ```bash
   node -e "import('./gsd/scripts/deduplicator.js').then(async m => {
     const findings = [
       { source: 'url1', content: 'Hello World', title: 'T1' },
       { source: 'url2', content: 'hello  world', title: 'T2' }
     ];
     const result = m.deduplicateFindings(findings);
     console.log('Count:', result.length);
     console.log('Alternates:', result[0].alternateSources);
   })"
   ```
   Expected: Count: 1, Alternates: [{ url: 'url2', title: 'T2' }]
</verification>

<success_criteria>
1. source-validator.js uses regex patterns (not string matching) for authority tiers
2. AUTHORITY_RULES constant defines HIGH/MEDIUM/LOW tier patterns
3. classifySourceAuthority returns tier name or UNVERIFIED
4. assignConfidenceLevel combines authority + metadata for final confidence
5. deduplicator.js hashes content with SHA256 (not MD5 or simple hash)
6. hashContent normalizes: lowercase + collapse whitespace + trim
7. deduplicateFindings uses Map for seen tracking (efficient lookup)
8. Alternate sources preserved with url + title metadata
9. Deduplication stats logged to console
10. Both modules use ESM syntax (import/export)
</success_criteria>

<output>
After completion, create `.planning/phases/07-enhanced-research-infrastructure/07-02-SUMMARY.md`

Summary should document:
- source-validator.js authority tier definitions and regex patterns
- deduplicator.js normalization and hashing approach
- Example classifications for common documentation sites
- Deduplication effectiveness (test with duplicate content examples)
- Integration points with research-synthesizer.js
- Any edge cases discovered during testing
</output>
