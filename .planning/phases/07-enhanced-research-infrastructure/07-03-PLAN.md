---
phase: 07-enhanced-research-infrastructure
plan: 03
type: execute
wave: 2
depends_on: [07-01, 07-02]
files_modified:
  - gsd/scripts/domain-coordinator.js
  - gsd/scripts/researcher.js
autonomous: true

must_haves:
  truths:
    - "System executes STACK/FEATURES/ARCHITECTURE/PITFALLS research in parallel"
    - "Concurrency is limited to prevent resource exhaustion"
    - "researcher.js uses real scraping instead of mock data"
    - "Real web content is fetched, parsed, and validated"
  artifacts:
    - path: "gsd/scripts/domain-coordinator.js"
      provides: "Parallel multi-domain research coordination"
      exports: ["coordinateMultiDomainResearch", "performDomainResearch"]
      min_lines: 150
    - path: "gsd/scripts/researcher.js"
      provides: "Real web scraping integration (no mocks)"
      contains: "scrapeContent"
      not_contains: "generateMockSearchResults"
  key_links:
    - from: "gsd/scripts/domain-coordinator.js"
      to: "p-limit"
      via: "concurrency control for parallel domains"
      pattern: "pLimit\\([0-9]+\\)"
    - from: "gsd/scripts/researcher.js"
      to: "scraper.js"
      via: "scrapeContent() replaces generateMockSearchResults()"
      pattern: "import.*scraper"
    - from: "gsd/scripts/domain-coordinator.js"
      to: "researcher.js"
      via: "performResearch() calls for each domain"
      pattern: "performResearch.*domain"
---

<objective>
Enable parallel multi-domain research execution and integrate real web scraping.

Purpose: Replace sequential mock research with parallel execution across STACK/FEATURES/ARCHITECTURE/PITFALLS domains using real web scraping. This delivers the 4x speedup identified in research and removes mock data dependency.

Output: domain-coordinator.js for parallel execution and updated researcher.js using scraper.js instead of generateMockSearchResults().
</objective>

<execution_context>
@~/.claude/get-stuff-done/workflows/execute-plan.md
@~/.claude/get-stuff-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-enhanced-research-infrastructure/07-RESEARCH.md

Dependencies from Prior Plans:
@.planning/phases/07-enhanced-research-infrastructure/07-01-PLAN.md (scraper.js)
@.planning/phases/07-enhanced-research-infrastructure/07-02-PLAN.md (source-validator.js, deduplicator.js)

Phase 4 Current State:
@.planning/phases/04-advanced-features/04-03-SUMMARY.md
- researcher.js has performResearch(), extractFindings(), mergeManualFindings()
- Currently uses generateMockSearchResults() with TODO for real web search
- Query generation already works (STACK/FEATURES/ARCHITECTURE/PITFALLS)

Phase 6 Context Integration:
@.planning/phases/06-discussion-and-context-system/06-02-SUMMARY.md
- context-loader.js can load locked decisions from CONTEXT.md
- Research should respect user constraints

Research Patterns:
- Pattern 2: Multi-Domain Parallel Execution (p-limit for concurrency)
- Pattern 4: Pitfall - Over-Parallelism (limit to 3-5 browsers, 10-20 HTTP)
</context>

<tasks>

<task type="auto">
  <name>Create domain-coordinator.js for parallel execution</name>
  <files>gsd/scripts/domain-coordinator.js</files>
  <action>
Create domain-coordinator.js implementing parallel multi-domain research from 07-RESEARCH.md:

**Export 2 functions:**

1. **performDomainResearch(topic, domain, options = {})** - Single domain research
   - Import performResearch from researcher.js
   - Import loadPhaseContext from context-loader.js
   - If options.phase and options.phaseName provided:
     - Load context: const context = await loadPhaseContext(phase, phaseName)
     - Check for locked decisions in context.decisions
     - If locked technology_stack exists, use it to constrain topic
     - Log: "Respecting locked decision: {key} = {value}"
   - Call performResearch(topic, domain, options)
   - Return findings array with domain metadata

2. **coordinateMultiDomainResearch(topic, options = {})** - Parallel coordinator
   - Import pLimit from 'p-limit'
   - Define domains array: ['STACK', 'FEATURES', 'ARCHITECTURE', 'PITFALLS']
   - Set concurrency limit: const limit = pLimit(options.concurrency || 2)
     - Default to 2 (conservative - can be increased in config)
     - See Pitfall 4 in research: over-parallelism causes EMFILE errors
   - Create domain promises with pLimit wrapper:
     ```javascript
     const domainPromises = domains.map(domain =>
       limit(async () => {
         console.log(`[${domain}] Starting research for: ${topic}`);
         const findings = await performDomainResearch(topic, domain, options);
         console.log(`[${domain}] Found ${findings.length} sources`);
         return { domain, findings };
       })
     );
     ```
   - Execute in parallel: const results = await Promise.all(domainPromises)
   - Transform to object: `{ stack: [...], features: [...], architecture: [...], pitfalls: [...] }`
   - Return domain results object

**Implementation requirements:**
- ESM module: Use import/export syntax
- Logging: console.log() for progress tracking per domain
- Error handling: Wrap domain research in try-catch, continue on individual failures
- Concurrency: Default to 2, configurable via options.concurrency
- Context-aware: Load CONTEXT.md if phase provided, respect locked decisions
- Integration: Import from researcher.js (performResearch) and context-loader.js

**Concurrency rationale (from research):**
- Scraping is I/O-bound (waiting for network), not CPU-bound
- 2 concurrent domains = safe default (4 total with STACK+FEATURES parallel)
- Can increase to 3-4 if no rate limiting issues
- Pitfall 4: Over-parallelism (>5 browsers) causes memory issues

Reference: 07-RESEARCH.md "Pattern 2: Multi-Domain Parallel Execution"
  </action>
  <verify>
```bash
# Check exports
grep -E "export (async )?function (coordinateMultiDomainResearch|performDomainResearch)" gsd/scripts/domain-coordinator.js

# Check p-limit import and usage
grep "import.*pLimit.*p-limit" gsd/scripts/domain-coordinator.js
grep "pLimit(" gsd/scripts/domain-coordinator.js

# Check context-loader integration
grep "import.*loadPhaseContext.*context-loader" gsd/scripts/domain-coordinator.js

# Check researcher.js integration
grep "import.*performResearch.*researcher" gsd/scripts/domain-coordinator.js

# Check logging for progress tracking
grep "console.log.*domain.*Starting research" gsd/scripts/domain-coordinator.js
```

All imports and usage patterns should be present.
  </verify>
  <done>
domain-coordinator.js exists with 2 exported functions. p-limit used for concurrency control (default 2). Context-aware research respects locked decisions from CONTEXT.md. Progress logging per domain. Error handling for individual domain failures. File is 150+ lines.
  </done>
</task>

<task type="auto">
  <name>Update researcher.js to use real scraping</name>
  <files>gsd/scripts/researcher.js</files>
  <action>
Update researcher.js to replace mock data with real web scraping:

**Changes to make:**

1. **Add imports at top:**
   ```javascript
   import { scrapeContent } from './scraper.js';
   import { classifySourceAuthority } from './source-validator.js';
   import { deduplicateFindings } from './deduplicator.js';
   ```

2. **Update performResearch() function:**
   - Remove call to generateMockSearchResults()
   - Add real web scraping logic:
     ```javascript
     // Generate search queries (already exists)
     const queries = generateSearchQueries(topic, type);

     // For MVP: Use first query to build documentation URL
     // In production with WebSearch: execute all queries
     const docUrls = buildDocumentationUrls(topic, type);

     // Scrape each URL
     const scrapedFindings = [];
     for (const url of docUrls) {
       try {
         const result = await scrapeContent(url);
         scrapedFindings.push({
           title: result.title || topic,
           url: result.url,
           content: result.content.substring(0, 500), // First 500 chars
           snippet: result.content.substring(0, 200),
           method: result.method
         });
       } catch (error) {
         console.warn(`Failed to scrape ${url}: ${error.message}`);
       }
     }

     return scrapedFindings;
     ```

3. **Add buildDocumentationUrls() helper:**
   - For common topics, build direct documentation URLs:
   - React: https://react.dev/
   - Node.js: https://nodejs.org/en/docs/
   - Express: https://expressjs.com/
   - Pattern: `https://docs.${topic.toLowerCase()}.{com|dev|org}`
   - Return array of candidate URLs to scrape

4. **Update extractFindings():**
   - Keep HTTPS filtering (already exists)
   - Keep forum filtering (already exists)
   - Add deduplication call:
     ```javascript
     const deduplicated = deduplicateFindings(filtered);
     return deduplicated;
     ```

5. **Update assignConfidenceLevel usage:**
   - Replace existing simple logic with source-validator.js:
   ```javascript
   import { assignConfidenceLevel } from './source-validator.js';
   // Use in extractFindings or mergeManualFindings
   findings.forEach(f => {
     f.confidence = assignConfidenceLevel(f);
   });
   ```

6. **Remove or comment out generateMockSearchResults():**
   - Add comment: "// DEPRECATED: Replaced with real scraping via scraper.js"
   - Keep function for backward compatibility in tests (or remove if tests updated)

**Important notes:**
- This is MVP implementation using direct documentation URLs
- Full WebSearch integration can be added later when available in Tabnine
- buildDocumentationUrls() is heuristic-based (works for popular libraries)
- Future enhancement: Use WebSearch API to get URLs, then scrape them

**Do NOT:**
- Remove generateSearchQueries() - still useful for WebSearch integration
- Remove existing filtering logic (HTTPS, forums) - still needed
- Break existing tests - update tests in Plan 4 if needed
  </action>
  <verify>
```bash
# Check new imports
grep "import.*scrapeContent.*scraper" gsd/scripts/researcher.js
grep "import.*classifySourceAuthority.*source-validator" gsd/scripts/researcher.js
grep "import.*deduplicateFindings.*deduplicator" gsd/scripts/researcher.js

# Check buildDocumentationUrls helper and its usage in loop
grep "buildDocumentationUrls" gsd/scripts/researcher.js
grep -A5 'buildDocumentationUrls' gsd/scripts/researcher.js | grep 'scrapeContent'

# Check deduplicateFindings usage
grep "deduplicateFindings(" gsd/scripts/researcher.js

# Verify mock is removed/deprecated
grep -E "(DEPRECATED|removed).*generateMockSearchResults" gsd/scripts/researcher.js || echo "Mock function still active - verify if intentional"
```

Imports should exist, scraping integrated, deduplication added, buildDocumentationUrlsâ†’scrapeContent wiring verified.
  </verify>
  <done>
researcher.js updated with real scraping imports. performResearch() uses scrapeContent() instead of generateMockSearchResults(). buildDocumentationUrls() helper added for MVP documentation URL building. extractFindings() calls deduplicateFindings(). assignConfidenceLevel integrated from source-validator.js. Mock function deprecated with comment. Wiring confirmed: buildDocumentationUrls generates URLs, loop calls scrapeContent on each URL.
  </done>
</task>

</tasks>

<verification>
Manual verification after task completion:

1. domain-coordinator.js parallel execution:
   ```bash
   node -e "import('./gsd/scripts/domain-coordinator.js').then(async m => {
     const results = await m.coordinateMultiDomainResearch('React', { concurrency: 2 });
     console.log('Domains researched:', Object.keys(results));
     console.log('STACK findings:', results.stack?.length || 0);
   })"
   ```
   Should show all 4 domains with parallel execution logs.

2. researcher.js real scraping:
   ```bash
   node -e "import('./gsd/scripts/researcher.js').then(async m => {
     const findings = await m.performResearch('Node.js', 'STACK');
     console.log('Findings:', findings.length);
     console.log('First finding method:', findings[0]?.method);
   })"
   ```
   Should return findings with method: 'static' or 'dynamic' (not mock data).

3. Context awareness test (optional):
   Create test CONTEXT.md with locked decision, verify domain-coordinator respects it.
</verification>

<success_criteria>
1. domain-coordinator.js exports coordinateMultiDomainResearch and performDomainResearch
2. p-limit imported and used with default concurrency of 2
3. Parallel execution logs show domain progress ("Starting research", "Found N sources")
4. Context-aware research loads CONTEXT.md and respects locked decisions
5. researcher.js imports scraper.js, source-validator.js, deduplicator.js
6. performResearch() uses scrapeContent() instead of generateMockSearchResults()
7. buildDocumentationUrls() helper generates candidate URLs
8. extractFindings() calls deduplicateFindings() to remove duplicates
9. assignConfidenceLevel() uses enhanced source-validator.js logic
10. Mock function marked as DEPRECATED or removed
11. Error handling preserves partial results (one domain failure doesn't stop others)
</success_criteria>

<output>
After completion, create `.planning/phases/07-enhanced-research-infrastructure/07-03-SUMMARY.md`

Summary should document:
- domain-coordinator.js parallel execution pattern and concurrency limits
- researcher.js integration with scraper.js, source-validator.js, deduplicator.js
- buildDocumentationUrls() heuristic approach and coverage
- Mock data removal and backward compatibility handling
- Context-aware research integration with CONTEXT.md
- Performance characteristics (parallel speedup observed)
- Any issues with specific documentation sites (scraping failures)
- Future enhancement path for WebSearch integration
</output>
