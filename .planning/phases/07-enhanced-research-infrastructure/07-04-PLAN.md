---
phase: 07-enhanced-research-infrastructure
plan: 04
type: execute
wave: 3
depends_on: [07-03]
files_modified:
  - gsd/scripts/integration-test.js
autonomous: false

must_haves:
  truths:
    - "All scraper.js functions pass automated tests"
    - "All source-validator.js functions pass automated tests"
    - "All deduplicator.js functions pass automated tests"
    - "All domain-coordinator.js functions pass automated tests"
    - "End-to-end research workflow produces real scraped content"
    - "User verifies research quality with real documentation sites"
  artifacts:
    - path: "gsd/scripts/integration-test.js"
      provides: "Test Suites 13 and 14 for Phase 7 modules"
      contains: "Test Suite 13: Web Scraping"
      contains: "Test Suite 14: Multi-Domain Coordination"
      min_lines: 75
  key_links:
    - from: "Test Suite 13"
      to: "scraper.js, source-validator.js, deduplicator.js"
      via: "unit tests for each exported function"
      pattern: "scrapeContent|classifySourceAuthority|deduplicateFindings"
    - from: "Test Suite 14"
      to: "domain-coordinator.js, researcher.js"
      via: "integration tests for parallel execution"
      pattern: "coordinateMultiDomainResearch|performResearch"
---

<objective>
Validate all Phase 7 modules with automated tests and end-to-end research verification.

Purpose: Ensure scraper.js, source-validator.js, deduplicator.js, domain-coordinator.js, and updated researcher.js all work correctly individually and together. Verify research quality with real documentation sites.

Output: Test Suites 13 and 14 added to integration-test.js with 100% pass rate. User verification checkpoint for research quality.
</objective>

<execution_context>
@~/.claude/get-stuff-done/workflows/execute-plan.md
@~/.claude/get-stuff-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-enhanced-research-infrastructure/07-RESEARCH.md

Phase 7 Modules to Test:
@.planning/phases/07-enhanced-research-infrastructure/07-01-PLAN.md (scraper.js)
@.planning/phases/07-enhanced-research-infrastructure/07-02-PLAN.md (source-validator.js, deduplicator.js)
@.planning/phases/07-enhanced-research-infrastructure/07-03-PLAN.md (domain-coordinator.js, researcher.js updates)

Phase 2 Testing Pattern:
@.planning/phases/02-core-infrastructure/02-05-SUMMARY.md
- integration-test.js with multiple test suites
- 100% pass rate requirement
- Accumulate failures, comprehensive reporting

Current Test Count:
- STATE.md shows 66/66 tests passing (Phase 1-6)
- Phase 7 will add ~15-20 tests (Suites 13-14)
</context>

<tasks>

<task type="auto">
  <name>Add Test Suite 13 for web scraping modules</name>
  <files>gsd/scripts/integration-test.js</files>
  <action>
Add Test Suite 13 to integration-test.js following existing test suite patterns:

**Test Suite 13: Web Scraping (9 tests)**

Import Phase 7 modules at top:
```javascript
import { scrapeContent, scrapeWithFallback, fetchWithRetry } from './scraper.js';
import { classifySourceAuthority, assignConfidenceLevel } from './source-validator.js';
import { deduplicateFindings, hashContent } from './deduplicator.js';
```

**Tests for scraper.js (3 tests):**

1. **Test: scrapeContent returns content object with method/content/title**
   - Call scrapeContent('https://nodejs.org/en/docs/')
   - Assert result has properties: method, content, title, url
   - Assert method is 'static' or 'dynamic'
   - Assert content.length > 100

2. **Test: fetchWithRetry respects maxRetries parameter**
   - Mock axios to fail 2 times, succeed on 3rd
   - Call fetchWithRetry(mockUrl, 3)
   - Assert function retried and succeeded
   - Assert did not exceed maxRetries

3. **Test: scrapeWithFallback tries static first**
   - Monitor which method used (check console.log or return value)
   - Assert Cheerio attempted before Playwright
   - (Can use mock URLs or real static documentation site)

**Tests for source-validator.js (3 tests):**

4. **Test: classifySourceAuthority returns HIGH for official docs**
   - Test 'https://docs.react.dev/' → HIGH
   - Test 'https://nodejs.org/en/docs/' → HIGH
   - Test 'https://github.com/facebook/react/docs/' → HIGH

5. **Test: classifySourceAuthority returns MEDIUM for MDN/StackOverflow**
   - Test 'https://developer.mozilla.org/en-US/' → MEDIUM
   - Test 'https://stackoverflow.com/questions/' → MEDIUM

6. **Test: classifySourceAuthority returns UNVERIFIED for unknown domains**
   - Test 'https://random-blog.example.com/' → UNVERIFIED
   - Test 'https://unknown.xyz/' → UNVERIFIED

**Tests for deduplicator.js (3 tests):**

7. **Test: hashContent normalizes whitespace and case**
   - Hash "Hello  World" and "hello world" → same hash
   - Hash "Test\n\n\nString" and "test string" → same hash

8. **Test: deduplicateFindings removes exact duplicates**
   - Create findings: [{ content: 'A' }, { content: 'A' }, { content: 'B' }]
   - Result should have 2 findings (A and B)

9. **Test: deduplicateFindings merges alternate sources**
   - Create findings: [{ source: 'url1', content: 'Same' }, { source: 'url2', content: 'Same' }]
   - Result[0] should have alternateSources array with url2

**Error handling:**
- Wrap network-dependent tests (scrapeContent) in try-catch
- Skip or mark as warning if network unavailable
- All other tests should work offline

Reference existing test suites (Test Suite 1-12) for patterns and reporting format.
  </action>
  <verify>
```bash
# Check Test Suite 13 exists
grep "Test Suite 13: Web Scraping" gsd/scripts/integration-test.js

# Check imports
grep -E "import.*scraper|source-validator|deduplicator" gsd/scripts/integration-test.js

# Run tests
cd gsd && npm test 2>&1 | grep -A20 "Test Suite 13"
```

Test Suite 13 should show 9 tests with results.
  </verify>
  <done>
Test Suite 13 added to integration-test.js with 9 tests covering scraper.js (3), source-validator.js (3), deduplicator.js (3). All tests pass or are appropriately handled for network dependency. Test count increased from 66 to 75.
  </done>
</task>

<task type="auto">
  <name>Add Test Suite 14 for multi-domain coordination</name>
  <files>gsd/scripts/integration-test.js</files>
  <action>
Add Test Suite 14 to integration-test.js:

**Test Suite 14: Multi-Domain Coordination (6 tests)**

Import domain-coordinator and updated researcher:
```javascript
import { coordinateMultiDomainResearch, performDomainResearch } from './domain-coordinator.js';
import { performResearch } from './researcher.js';
```

**Tests for domain-coordinator.js (3 tests):**

1. **Test: coordinateMultiDomainResearch executes all 4 domains**
   - Call coordinateMultiDomainResearch('React', { concurrency: 2 })
   - Assert result has keys: stack, features, architecture, pitfalls
   - Assert each domain has findings array
   - (May have empty arrays if scraping fails - that's okay for test)

2. **Test: performDomainResearch returns findings with domain metadata**
   - Call performDomainResearch('Node.js', 'STACK')
   - Assert result is array
   - Assert findings have required properties (title, url, content)

3. **Test: p-limit controls concurrency**
   - Mock pLimit to track concurrent calls
   - Verify concurrency limit respected (default 2)
   - Assert no more than 2 domains execute simultaneously

**Tests for updated researcher.js (3 tests):**

4. **Test: performResearch uses real scraping (not mock data)**
   - Call performResearch('Express', 'STACK')
   - Assert findings have 'method' property (from scraper.js)
   - Assert findings do NOT have mock data markers

5. **Test: performResearch applies deduplication**
   - If possible, create scenario with duplicate content
   - Verify deduplicateFindings was called (check results)
   - Assert no duplicate content in results

6. **Test: extractFindings integrates source-validator confidence**
   - Call performResearch and check findings
   - Assert findings have 'confidence' property
   - Assert confidence is HIGH/MEDIUM/LOW/UNVERIFIED (not simple domain check)

**Integration test (end-to-end):**
- Optional: Test full workflow from coordinateMultiDomainResearch → research-synthesizer.js → document generation
- Verify complete pipeline works with real scraping

**Network handling:**
- Wrap network-dependent tests in try-catch
- Mark as skipped or warning if network unavailable
- Consider using localhost test server for deterministic tests

Update test summary at end to reflect total test count (should be 81: 66 existing + 9 Suite 13 + 6 Suite 14).
  </action>
  <verify>
```bash
# Check Test Suite 14 exists
grep "Test Suite 14: Multi-Domain Coordination" gsd/scripts/integration-test.js

# Check imports
grep -E "import.*(coordinateMultiDomainResearch|performDomainResearch)" gsd/scripts/integration-test.js

# Run tests
cd gsd && npm test 2>&1 | grep -A20 "Test Suite 14"

# Check total test count
cd gsd && npm test 2>&1 | tail -5
```

Test Suite 14 should show 6 tests. Total should be 81 tests (or 75+ if network tests skipped).
  </verify>
  <done>
Test Suite 14 added to integration-test.js with 6 tests covering domain-coordinator.js (3) and researcher.js integration (3). Network-dependent tests handle failures gracefully. Total test count is 81 (66 + 9 + 6). All tests pass or marked as warnings for network issues.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 7 Enhanced Research Infrastructure with:
- scraper.js: Progressive enhancement web scraping (Cheerio → Playwright fallback)
- source-validator.js: Multi-tier authority classification (HIGH/MEDIUM/LOW/UNVERIFIED)
- deduplicator.js: Content-based duplicate detection with SHA256 hashing
- domain-coordinator.js: Parallel multi-domain research with p-limit concurrency control
- Updated researcher.js: Real scraping replacing mock data, integrated with all Phase 7 modules
- Test Suites 13 & 14: 15 new tests validating all modules (81 total tests)
  </what-built>
  <how-to-verify>
**Step 1: Run automated tests**
```bash
cd gsd
npm test
```
Verify: All 81 tests pass (or 75+ if network tests skipped/warning). Test Suites 13 and 14 show green checkmarks.

**Step 2: Test real research workflow**
```bash
node -e "
import('./scripts/domain-coordinator.js').then(async m => {
  console.log('Starting multi-domain research for React...');
  const results = await m.coordinateMultiDomainResearch('React', { concurrency: 2 });
  console.log('\\nResults:');
  console.log('STACK findings:', results.stack?.length || 0);
  console.log('FEATURES findings:', results.features?.length || 0);
  console.log('ARCHITECTURE findings:', results.architecture?.length || 0);
  console.log('PITFALLS findings:', results.pitfalls?.length || 0);
  console.log('\\nFirst STACK finding:');
  console.log(results.stack?.[0]);
  process.exit(0);
});
"
```
Verify:
- All 4 domains show findings counts (may be 0-5 depending on scraping success)
- First finding has: title, url, content, method ('static' or 'dynamic'), confidence
- No mock data present (no "Mock Finding" in titles)
- Parallel execution logs show domains starting concurrently

**Step 3: Test source validation**
```bash
node -e "
import('./scripts/source-validator.js').then(m => {
  console.log('Testing source validation:');
  console.log('React docs:', m.classifySourceAuthority('https://react.dev/'));
  console.log('MDN:', m.classifySourceAuthority('https://developer.mozilla.org/'));
  console.log('Medium blog:', m.classifySourceAuthority('https://medium.com/article'));
  console.log('Unknown site:', m.classifySourceAuthority('https://random.com/'));
  process.exit(0);
});
"
```
Verify: Outputs HIGH, MEDIUM, LOW, UNVERIFIED (in that order)

**Step 4: Test deduplication**
```bash
node -e "
import('./scripts/deduplicator.js').then(m => {
  const findings = [
    { source: 'url1', content: 'React is a library', title: 'T1' },
    { source: 'url2', content: 'react  is a library', title: 'T2' },
    { source: 'url3', content: 'Different content', title: 'T3' }
  ];
  const result = m.deduplicateFindings(findings);
  console.log('Original:', findings.length, 'Deduplicated:', result.length);
  console.log('Alternate sources:', result[0].alternateSources);
  process.exit(0);
});
"
```
Verify: 3 → 2 findings, first has alternateSources: [{ url: 'url2', title: 'T2' }]

**Step 5: Verify scraper progressive enhancement**
```bash
node -e "
import('./scripts/scraper.js').then(async m => {
  console.log('Testing scraper on Node.js docs (should be static)...');
  const result = await m.scrapeContent('https://nodejs.org/en/docs/');
  console.log('Method:', result.method);
  console.log('Title:', result.title);
  console.log('Content length:', result.content.length);
  process.exit(0);
});
"
```
Verify: Method is 'static' (Cheerio succeeded, no Playwright fallback), content length > 1000

**Step 6: Test context-aware research (if CONTEXT.md exists)**
If a phase has CONTEXT.md with locked technology_stack decision:
```bash
node -e "
import('./scripts/domain-coordinator.js').then(async m => {
  const results = await m.performDomainResearch('React', 'STACK', {
    phase: '07',
    phaseName: 'enhanced-research-infrastructure'
  });
  console.log('Context-aware research results:', results.length);
  process.exit(0);
});
"
```
Verify: Logs show "Respecting locked decision" if CONTEXT.md has technology_stack

**Quality checks:**
1. Research findings are real scraped content (not mock data)
2. Source validation correctly classifies authority tiers
3. Deduplication catches near-duplicates with different URLs
4. Parallel execution completes faster than sequential (4 domains in ~2x time, not 4x)
5. Browser cleanup works (no memory leaks - check process doesn't hang)
6. Rate limiting respected (no 429 errors from rapid scraping)
  </how-to-verify>
  <resume-signal>
Type "approved" if all verification steps pass and research quality is good.

If issues found, describe them (e.g., "scraping fails on X site", "deduplication not working", "rate limiting errors").
  </resume-signal>
</task>

</tasks>

<verification>
Overall phase verification:

1. All 81 integration tests pass (66 existing + 15 new)
2. Real web scraping works (no mock data in results)
3. Progressive enhancement uses Cheerio first (faster)
4. Source validation uses multi-tier authority classification
5. Deduplication catches duplicate content across different URLs
6. Parallel execution completes all 4 domains concurrently
7. Context-aware research respects locked decisions from CONTEXT.md
8. No memory leaks from unclosed browsers
9. Rate limiting handled gracefully with exponential backoff
10. Integration with existing Phase 4 infrastructure preserved
</verification>

<success_criteria>
1. scraper.js successfully scrapes real documentation sites
2. source-validator.js classifies sources into 4 tiers (HIGH/MEDIUM/LOW/UNVERIFIED)
3. deduplicator.js removes duplicate content and merges alternate sources
4. domain-coordinator.js executes 4 domains in parallel with controlled concurrency
5. researcher.js no longer uses mock data (generateMockSearchResults removed/deprecated)
6. Test Suite 13 validates all web scraping modules (9 tests pass)
7. Test Suite 14 validates multi-domain coordination (6 tests pass)
8. Total test count is 81 with 100% pass rate
9. End-to-end research workflow produces real scraped findings
10. User verifies research quality meets expectations
11. No regressions in existing Phase 4/6 functionality
12. Browser cleanup prevents memory leaks
13. Rate limiting errors handled gracefully
14. Context-aware research integration works
</success_criteria>

<output>
After completion, create `.planning/phases/07-enhanced-research-infrastructure/07-04-SUMMARY.md`

Summary should document:
- Test Suites 13 and 14 structure and coverage
- Total test count (81) and pass rate (100%)
- End-to-end research workflow validation results
- Research quality assessment (real vs mock data comparison)
- Performance characteristics (parallel speedup, scraping times)
- Any scraping failures or site compatibility issues
- Memory and resource usage observations
- Integration with Phase 4 (researcher.js) and Phase 6 (context-loader.js)
- User verification feedback and any issues found
- Recommendations for future enhancements (WebSearch integration, proxy rotation)
</output>
